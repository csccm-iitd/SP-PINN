{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "%reset -f\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('u_sol_poissons1.mat')  \t# Load data from file\n",
    "x = data['x1d']                                   # 200 points between -1 and 1 [256x1]\n",
    "y = data['y1d']                                   # 200 time points between 0 and 1 [100x1] \n",
    "usol = data['zd']  \n",
    "X, Y = np.meshgrid(x,y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' X_u_test = [X[i],T[i]] [25600,2] for interpolation'''\n",
    "X_u_test = np.hstack((X.flatten()[:,None], Y.flatten()[:,None]))\n",
    "\n",
    "# Domain bounds\n",
    "lb = X_u_test[0]  # [-1. -1.]\n",
    "ub = X_u_test[-1] # [1.  1]\n",
    "\n",
    "'''\n",
    "   Fortran Style ('F') flatten,stacked column wise!\n",
    "   u = [c1 \n",
    "        c2\n",
    "        .\n",
    "        .\n",
    "        cn]\n",
    "\n",
    "   u =  [25600x1] \n",
    "'''\n",
    "u_true = usol.flatten('F')[:,None] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_u,N_f):\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "\n",
    "    #Topedge condition 1\n",
    "    topedge_x = np.hstack((X[0,:][:,None], Y[0,:][:,None])) #L3\n",
    "    topedge_u = usol[0,:][:,None]\n",
    "    \n",
    "    #Bottom edge condition 1\n",
    "    bottemedge_x = np.hstack((X[0,:][:,None], Y[-1,:][:,None])) #L1\n",
    "    bottemedge_u = usol[-1,:][:,None]\n",
    "\n",
    "    #Left edge Condition 1\n",
    "    leftedge_x = np.hstack((X[:,0][:,None], Y[:,0][:,None])) #L2\n",
    "    leftedge_u = usol[:,0][:,None]\n",
    "    \n",
    "    #Right edge Condition 2\n",
    "    rightedge_x = np.hstack((X[:,-1][:,None], Y[:,0][:,None])) #L4\n",
    "    rightedge_u = usol[:,-1][:,None]\n",
    "\n",
    "\n",
    "    all_X_u_train = np.vstack([topedge_x, bottemedge_x, leftedge_x, rightedge_x]) # X_u_train [200,2] (800 = 200(L1)+200(L2)+200(L3)+200(L4))\n",
    "    all_u_train = np.vstack([topedge_u, bottemedge_u, leftedge_u, rightedge_u])   #corresponding u [800x1]\n",
    "\n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(all_X_u_train.shape[0], N_u, replace=False) \n",
    "\n",
    "    X_u_train = all_X_u_train[idx, :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = all_u_train[idx,:]      #choose corresponding u\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    #sampler = qmc.Sobol(d=2, scramble=False)\n",
    "    #sample=sampler.random_base2(m=15)\n",
    "    #X_f_train = qmc.scale(sample, lb, ub) \n",
    "    #X_f_train = lb + (ub-lb)*np.random.rand(N_f,2) \n",
    "    \n",
    "    xt1=lb[0] + (ub[0]-lb[0])*np.linspace(0,1,N_f)\n",
    "    yt1=lb[1] + (ub[1]-lb[1])*np.linspace(0,1,N_f)\n",
    "    Xt1, Yt1 = np.meshgrid(xt1,yt1)\n",
    "    X_f_train = np.hstack([Xt1.reshape(N_f*N_f,1),Yt1.reshape(N_f*N_f,1)])\n",
    "\n",
    "    #X_f_train = np.vstack((X_f_train, X_u_train)) # append training points to collocation points \n",
    "    \n",
    "    return X_f_train, X_u_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neighbouring point index\n",
    "def neighbour_index(xx):\n",
    "    nr = 5\n",
    "    zn = torch.zeros([xx.shape[0],nr],dtype=torch.int32)\n",
    "    for j in range(xx.shape[0]): \n",
    "        ngr1 = (np.ones([1,nr],dtype='int32')*j)[0].tolist()\n",
    "        x = xx[j,:][None,:]\n",
    "        n = x.shape[1]\n",
    "        ngr1_d= torch.where(torch.linalg.norm(xx-x,axis=1)<0.041)[0].tolist()\n",
    "        ngr1[0:len(ngr1_d)] = ngr1_d\n",
    "        zn[j,:] = torch.tensor(ngr1)[0:nr]\n",
    "    return zn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corresponding  inverse matrices\n",
    "def inverse_index(X_f_train):\n",
    "    nr = 5\n",
    "    zn = torch.zeros([X_f_train.shape[0],nr],dtype=torch.int32)\n",
    "    zmd = torch.zeros([X_f_train.shape[0],X_f_train.shape[1],X_f_train.shape[1]])\n",
    "    for j in range(X_f_train.shape[0]): \n",
    "            ngr1 = (np.ones([1,nr],dtype='int32')*j)[0].tolist()\n",
    "            x = X_f_train[j,:][None,:]\n",
    "            n = x.shape[1]\n",
    "            md = torch.zeros((n,n)).to(device);\n",
    "            ngr1_d= torch.where(torch.linalg.norm(X_f_train-x,axis=1)<0.041)[0].tolist()\n",
    "            ngr1[0:len(ngr1_d)] = ngr1_d\n",
    "            zn[j,:] = torch.tensor(ngr1)[0:nr]\n",
    "            xd = X_f_train[ngr1_d[0:nr],:][None,:]-x;\n",
    "            md = torch.einsum('abi,abj->aij',xd,xd)\n",
    "            md_inv = torch.inverse(md)\n",
    "            zmd[j,:,:] = md_inv \n",
    "    return zmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "    \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        self.iter = 0\n",
    "        \n",
    "        '''\n",
    "        Alternatively:\n",
    "        \n",
    "        *all layers are callable \n",
    "    \n",
    "        Simple linear Layers\n",
    "        self.fc1 = nn.Linear(2,50)\n",
    "        self.fc2 = nn.Linear(50,50)\n",
    "        self.fc3 = nn.Linear(50,50)\n",
    "        self.fc4 = nn.Linear(50,1)\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        'Xavier Normal Initialization'\n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            \n",
    "            # weights from a normal distribution with \n",
    "            # Recommended gain value for tanh = 5/3?\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            \n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        \n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "                        \n",
    "        '''     \n",
    "        Alternatively:\n",
    "        \n",
    "        a = self.activation(self.fc1(a))\n",
    "        a = self.activation(self.fc2(a))\n",
    "        a = self.activation(self.fc3(a))\n",
    "        a = self.fc4(a)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            \n",
    "            z = self.linears[i](a)\n",
    "                        \n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a)\n",
    "        \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,x,y):\n",
    "                \n",
    "        loss_u = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_u\n",
    "    \n",
    "    def grad1(self,xx,n_index,inv_mat):\n",
    "        zd = torch.zeros(xx.shape)\n",
    "        m = n_index.shape[1]\n",
    "        u_0 = self.u_fun(xx)\n",
    "        x_ngr = xx[n_index.tolist(),:]\n",
    "        u_ngr = u_0[n_index.tolist(),:]\n",
    "        x_d  = x_ngr-xx.unsqueeze(1).repeat(1,m,1)\n",
    "        u_d = u_ngr-u_0.unsqueeze(1).repeat(1,m,1)\n",
    "        u_ds = torch.sum(torch.einsum('ijp,ikp->ikp',(u_d.permute(0,2,1)),(x_d.permute(0,2,1))),dim=2)\n",
    "        zd = torch.einsum('kj,kji->ki',u_ds,inv_mat)\n",
    "        return zd\n",
    "    \n",
    "    def grad2(self,xx,u_x_t,n_index,inv_mat):\n",
    "        zdd = torch.zeros(u_x_t.shape[0], 2*u_x_t.shape[1])\n",
    "        n = u_x_t.shape[0]\n",
    "        m = n_index.shape[1]\n",
    "        x_ngr = xx[n_index.tolist(),:]\n",
    "        u_xngr = u_x_t[n_index.tolist(),:]\n",
    "        x_d  = x_ngr-xx.unsqueeze(1).repeat(1,m,1)\n",
    "        u_xd = u_xngr-u_x_t.unsqueeze(1).repeat(1,m,1)\n",
    "        u_xds = torch.sum(torch.einsum('ijr,ipr->ijpr',(u_xd.permute(0,2,1)),(x_d.permute(0,2,1))),dim=3)\n",
    "        zdd = torch.einsum('kij,kjl->kil',u_xds,inv_mat).reshape([n,4])\n",
    "        return zdd\n",
    "    \n",
    "    \n",
    "    def u_fun(self,x):\n",
    "        x = x.clone()\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def loss_PDE(self, x_to_train_f):\n",
    "        \n",
    "        nu = 0.01/np.pi\n",
    "                \n",
    "        x_1_f = x_to_train_f[:,[0]]\n",
    "        x_2_f = x_to_train_f[:,[1]]\n",
    "        \n",
    "        u = self.u_fun(x_to_train_f)\n",
    "        \n",
    "        u_x_y = self.grad1(x_to_train_f,n_index,inv_mat)\n",
    "                                \n",
    "        \n",
    "        u_xx_yy = self.grad2(u_x_y,x_to_train_f,n_index,inv_mat)                                                    \n",
    "                                                            \n",
    "        \n",
    "        u_xx = u_xx_yy[:,[0]]\n",
    "        u_yy = u_xx_yy[:,[3]]\n",
    "        \n",
    "       \n",
    "        f = u_xx + u_yy\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return  loss_f\n",
    "    \n",
    "    def loss(self,x,y,x_to_train_f):\n",
    "\n",
    "        loss_u = self.loss_BC(x,y)\n",
    "        loss_f = self.loss_PDE(x_to_train_f)\n",
    "        \n",
    "        loss_val = 40*loss_u + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(X_u_train, u_train, X_f_train)\n",
    "        \n",
    "        loss.backward()\n",
    "                \n",
    "        self.iter += 1\n",
    "        \n",
    "        if self.iter % 200== 0:\n",
    "\n",
    "            error_vec, _ = PINN.test()\n",
    "        \n",
    "            print(loss,error_vec)\n",
    "\n",
    "        return loss        \n",
    "    \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "                \n",
    "        u_pred = self.forward(X_u_test_tensor)\n",
    "        \n",
    "        error_vec = torch.linalg.norm((u-u_pred),2)/torch.linalg.norm(u,2)        # Relative L2 Norm of the error (Vector)\n",
    "        \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        u_pred = np.reshape(u_pred,(200,200),order='F')\n",
    "        \n",
    "        u_pred = u_pred.T\n",
    "                \n",
    "        return error_vec, u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=40, bias=True)\n",
      "    (1): Linear(in_features=40, out_features=80, bias=True)\n",
      "    (2): Linear(in_features=80, out_features=40, bias=True)\n",
      "    (3): Linear(in_features=40, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor(166.4524, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.6106, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(5.0467, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.4184, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(2.2140, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.4116, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.0871, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.4203, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.6281, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.4234, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4885, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.4238, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training time: 36.60\n",
      "Test Error: 1.42258\n"
     ]
    }
   ],
   "source": [
    "'Generate Training data'\n",
    "N_u = 400 #Total number of data points for 'u'\n",
    "N_f = 65 #Total number of collocation points \n",
    "X_f_train_np_array, X_u_train_np_array, u_train_np_array = trainingdata(N_u,N_f)\n",
    "X_f_train = torch.from_numpy(X_f_train_np_array).float().to(device)\n",
    "X_u_train = torch.from_numpy(X_u_train_np_array).float().to(device)\n",
    "u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "X_u_test_tensor = torch.from_numpy(X_u_test).float().to(device)\n",
    "n_index = neighbour_index(X_f_train).to(device)\n",
    "inv_mat = inverse_index(X_f_train).to(device)\n",
    "u = torch.from_numpy(u_true).float().to(device)\n",
    "f_hat = torch.zeros(X_f_train.shape[0],1).to(device)\n",
    "layers = np.array([2,40,80,40,1]) #8 hidden layers\n",
    "\n",
    "PINN = Sequentialmodel(layers)\n",
    "\n",
    "PINN.to(device)\n",
    "\n",
    "'Neural Network Summary'\n",
    "print(PINN)\n",
    "\n",
    "params = list(PINN.parameters())\n",
    "'''Optimization'''\n",
    "\n",
    "'L-BFGS Optimizer'\n",
    "optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.1, \n",
    "                              max_iter = 1000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "optimizer.step(PINN.closure)\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.2f' % (elapsed))\n",
    "\n",
    "\n",
    "''' Model Accuracy ''' \n",
    "error_vec, u_pred = PINN.test()\n",
    "\n",
    "print('Test Error: %.5f'  % (error_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = 'poissons1.pt'\n",
    "path = F\"{model_save_name}\" \n",
    "torch.save(PINN.state_dict(), path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_save_name = 'poissons4225.pt'\n",
    "# path = F\"{model_save_name}\"\n",
    "# PINN.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: 1.42367\n"
     ]
    }
   ],
   "source": [
    "''' Model Accuracy ''' \n",
    "error_vec, u_pred = PINN.test()\n",
    "\n",
    "print('Test Error: %.5f'  % (error_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0668471984143102"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.sqrt((usol-u_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016026805558336122"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm((usol-u_pred),2)/np.linalg.norm(usol,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(u_pred, interpolation='nearest', cmap='jet',\n",
    "                extent=[X.min(), X.max(), Y.min(), Y.max()], \n",
    "                origin='lower')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(usol, interpolation='nearest', cmap='jet',\n",
    "                extent=[X.min(), X.max(), Y.min(), Y.max()], \n",
    "                origin='lower')\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
